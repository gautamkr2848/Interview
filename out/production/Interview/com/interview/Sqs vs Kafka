Messaging System Internal Data Striucture
    1. Queue Based DS - AWS SQS, Rabit MQ etc.
    2. Log Based DS - Kafka, AWS Kinesis etc.
        Log is the simplest data structure that represents append-only sequence of records. Log records are immutable
        and appended in strictly sequential order to the end of the log file.

Use Case of Messaging System
    1. Lightweight eventing / messaging between services or components
    2. Heavy processing of large stream of data

SQS vs Kafka

Amazon Simple Queue Service

    SQS is a managed message queuing service that technical professionals and developers use to send, store and
    retrieve multiple messages of various sizes all at the same time.

    Subsequently one service sends messages to the queue and another service receives them. However it can be used for many purposes.

    Importantly SQS frees developers from the hassle of configuring and managing queue structures as a managed service. Especially this service provides you with everything you need out of the box and is easily scalable to meet your demand.

    Significantly ideal solution for time critical projects and small to medium development teams.

    Working

        Send Message: Services send requests to queues that are used by other services.
        Receive Message: Another service requests to receive queued messages.
        Delete Message: Once the message is successfully processed, the user removes it from the queue, should he wishes to do so.

    Nevertheless SQS does not work as a database, so users cannot decide which messages in the queue to receive. However, you can limit the number of messages you will receive at any one time.

    When a message is received, SQS temporarily removes it from the queues so it is never sent twice. Moreover the message is hidden for a while and the user processes it and removse it from the list.

    If the message is not removed after the visibility timeout, it is then returned to the queue and receives a future requests. Timeout interval for visibility can be set.


    Features
        You are paying for the number of orders in the queue.
        Excellent service that improves the efficiency, reliability and performance of applications.
        Messages in the queue are delivered at least once. So guaranteed message delivery without message loss.
        And those messages that cannot be processed are saved in the garbage queue.
        There are two types of queues: standard queues and FIFO queues. In the default queue, messages are retrieved randomly.
        Multiple components run in a single queue. Besides SQS uses a blocking mechanism, if one component consumes a message, it  hides it from other components.
        After successful processing, the message is removed from the queue. If the message cannot be processed, it remains in the queue and is visible to all components. A feature called timeout visibility.
        No data loss.
        Each request is handled independently.


    Pros
        Automatic deduplication for FIFO queues.
        A separate queue for unprocessed messages.
        Scalability
        Low cost.


    Cons
        Lack of support for broadcast messages.
        High cost at scale.
        Reduced control over performance.


Kafka

    It’s a real technology that developers and architects use to build the latest generation of scalable streaming
    applications that serve real time data.

    Apache Kafka is an open source distributed messaging platform. Designed to manage streaming data in real time for
    distributed streaming, pipetting, and playback of data streams for fast and scalable operations.

    Apache Kafka is, of course, an optimized data warehouse in terms of data transfer and processing. The transferred
    data is continuously generated by thousands of data sources, typically by sending data records. Streaming platforms
    have to deal with the continuous flow of data, sequentially and incrementally.

    Kafka is a middleware based solution that works by maintaining data streams as logs on a series of servers. Moreover
    kafka servers span multiple data centers and provide data robustness by storing messages in threads across multiple
    server instances.

    Kafka is one of the fastest growing open source communications solutions. Design pattern gives you an excellent
    logging mechanism for distributed systems.

    Kafka is designed to stream logs in real time and is ideal for the needs of:

        Reliable data exchange between different components.
        Has built in support for data/message playback.
        Splits messaging workloads based on application needs.
        Real time transmission of data processing.


    Features
        Handles scalability in all four dimensions: event producers, event handlers, event consumers and event connectors. In other words, Kafka expands easily without stopping.
        Very productive for both publishing and subscribing messages. Maintains stable performance despite storing many terabytes of messages.
        Distributed, partitioned, replicated and fault tolerant, leads to amazing reliability.
        Forks new data streams using product data streams.
        Especially Kafka Mirror maker provides transcription support for your group.
        Replication messages are replicated across multiple data centers or cloud regions.
        You can use these passive/passive schemas for backup and recovery, or to bring data closer to users or to support data area requirements.
        Durable solution as it uses distributed commit log, that means messages persists on disk as fast as possible.
        Clusters of Kafka handle large failures and databases.


    Pros
        Very accessible.
        Reduces the need for multiple integrations.
        Low latency.
        Batch approach.
        Real time handling.
        High throughput.
        Distributed system.
        Operational Metrics/KPIs.
        Log aggregation.


    Cons
        Lacks some message paradigms.
        Reduces performance.
        Message tweaking issues.
        Not a complete set of monitoring tools.
        Do not support wildcard topic selection.
        Streaming ETL.


Kafka vs SQS - Key Differences

    Kafka is described in detail as a “fault tolerant and high performance publish subscribe messaging system.” So
    Kafka is a distributed, segmented and redundant commit log service. Provides the functionality of the email system
    but with a unique design.

    Max message size - 1 MB & configurable
    Message order always preserved
    Data retention is also configurable. Default 7 days.

    Amazon SQS as a “fully managed message queue service”. Hence you can transfer any amount of data without dropping
    messages or requiring other services to be always available. With SQS, you reduce the administrative burden of
    running and scaling a highly available message pool. While getting what you use at an affordable price.

    Max message size - 256 kb
    Message order always preserved only for FIFO queue
    Data retention 60 secs to 14 days


    Kafka Use cases

        Stream Data processing framework.
        Highly scalable system for large workloads that need to send messages in batches
        Topics in Kafka consists of multiple sections that are read in parallel by different consumers of a group of
        consumers, which gives us a very good performance.

    SQS Use cases

        Better suited for events, when you need to intercept a message (event) from the client, the message is
        automatically pulled from the queue.
        Not as fast as Kafka and not suitable for large workloads.
        Better for events with few events per second.

    Message Model - Apache Kafka follows publish subscriber model, where as SQS is pull based streaming.

    SQS has two types of queues: FIFO and Standard

    Message size - For Kafka is 1MB and configurable. But for SQS Max Message size 256KB.



Why is Kafka so fast?

Apache Kafka is an open-source distributed event streaming platform which is widely used for high performance pipelines, streaming of events etc.
Kafka comes with some core capabilities of providing a high throughput and a very low latency.
There are multiple design decisions that have been taken while designing Kafka to make it such highly performant. Out
of all, we will go through 2 very important factors that contribute to it:
    a. Sequential I/O
    b. Zero Copy Principle

Sequential I/O

One of the key factors that contributes to its performance is its use of sequential I/O. It uses sequential I/O to
improve the performance of its log-based storage system.

Kafka stores all the data it ingests in a log-structured format, which means that new data is appended to the end of the
log. This allows for very fast writes, since only the latest data needs to be appended, but it also means that older
data needs to be periodically compacted to reclaim disk space.

In addition to this, Kafka also uses a technique called log compaction to clean up old data, which is also done in a
sequential manner. It keeps the latest version of a message key, while discarding older versions, thus making use of
the sequential IO when doing compaction.

Zero-Copy Principle

The zero-copy principle is a technique used in computer systems to minimise the number of times data is copied between
different memory locations. This can significantly improve performance by reducing the amount of memory and CPU
resources used.

When data needs to be transferred from one location to another, a traditional approach would be to copy the data from
the source location to a temporary buffer, and then copy it again to the destination location. However, the zero-copy
principle eliminates this intermediate step by allowing the data to be transferred directly from the source location to
the destination location, without the need for an intermediate copy.

There are different ways to implement the zero-copy principle, but some common techniques include:
Memory mapping
Direct memory access (DMA)

Kafka uses the zero-copy principle to improve its performance by minimising the number of copies of data that are made
as messages are produced and consumed.

When a consumer reads a message from a Kafka topic, it can read the data directly from the log file, without the need
to copy it into a separate buffer. This is achieved using a technique called direct memory access (DMA). DMA allows the
consumer to read the data directly from the log file into the consumer’s memory buffer, without the need for an intermediate copy.
