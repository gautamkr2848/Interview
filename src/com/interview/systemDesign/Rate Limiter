
                                                           ---------------------
                                                           |    Rule Engine    |
                                                           ---------------------
                                                                    |
                                                                    |
                                                           ---------------------
                                                           |    Redis Cache    |
                                                           ---------------------
                                                                    ^
                                                                    |
                                                                   / \
                                                                /      \
    ---------------------        -----------------             /  Rate   \                 ---------------------
    |    Redis Cache    | <----> | Load Balancer | <---------> \  Limiter / <------------> |    API / Server   |
    ---------------------        -----------------            ^  \      /                  ---------------------
    ^                                                         |     \/
    |-------------          429           --------------------      |
                                                                    |
                                                          ---------------------
                                                          |    Redis Cache    |
                                                          ---------------------
                                                                    |
                                                                    |
                                                          ---------------------
                                                          | Logging Mechanism |
                                                          ---------------------
                                                                    |
                                                                    |
                                                          ------------------------
                                                          | Large Storage System |
                                                          ------------------------

What is Rate Limiting?

Rate limiter restricts the number of events that a certain user or device can perform within a given time range. It
helps us to limit the number of requests a sender can send in a perticular period of time. Once the upper limit is
reached, the rate limiter bolcks the further requests.


Why do Need Rate Limiting?

    1. DOS attack - When hackers try to flood a system with many requests within a short period of time.
    2. Service provider wants to use rate limiter for client based on pricing.
    3. If the application is not able to handle huge amount of requests.

Note - We can rate-limit the system in various ways, like based on user id, IP address, region, overall application wise etc.
        Rate limiter will throw, if any requests were rejected


Functional Requirements:

A client can send a limited number of requests to a server within a time window, e.g., 100 requests per minute. The
client should get an error message if the defined threshold limit of request is crossed.


Non-Functional Requirements:

    High availability
    Performance
    Rate limiter service should not add substantial latencies to the system


High-level Design:

When a new request arrives from the client, the Server first asks the Rate Limiter to decide if it will be served or
rejected. If the request is not rejected, then it’ll be passed to the internal API servers.

Algorithms:

    1. Token Bucket
    2. Leaky Bucket
    3. Fixed Window
    4. Sliding Window Log
    5. Silding Window Counter


Token Bucket

Assume a bucket has few tokens. When a request comes in, a token from the bucket must be taken and processed. The
request will be refused if no tokens is available in the bucket. As a result, token bucket gets refreshed every time
unit.

    Drawbacks - If there is a spike in traffic, you have large number of users who are getting response as 429 then you
    need to tweaking the size of bucket.


Leaky Bucket

Leaky bucket algorithm is based on the idea that if average rate of pouring water is greater than rate at which water
leaks, the bucket will overflow. This algorithm uses a queue, which will be used for holding requests. When a request
is submitted, it's added to queue's end. Additional requests are discarded, if the queue is full.


Fixed Window algorithm:

    A client can make two requests per minute. So, request 1 and request 2 are allowed by the rate limiter. But the
    third request is blocked as it can within 1 minute from the same user. This type of algorithm is named as Fixed
    Window algorithm. After 1 minute of the first request from a specific client, the start time is reset for that user.

    Drawbacks:

    It can allow twice the number of allowed requests per minute. Imagine the user sends two requests at the last
    second of a minute(12:01:59) and immediately makes two more requests at the first second(12:02:00) of the next
    minute. It may seem a minor problem for only two requests. But if the number of requests is much higher.


Sliding Window :

    Redis / HashTable -
        1. Log - userId as key & List of timestamp as value
        2. Counter - userId as key & List of timestamp with counter

    If we keep track of each request per user in a time frame, we may store the timestamp of each request in a Sorted
    Set in our ‘value’ field of hash-table. But, it would take a lot of memory. So, we can use a sliding window with
    the counters.


Problems in Distributed Environment:

    In the case of distributed systems, the algorithms we discussed will have some problems. A user is allowed to
    request 3 times within a minute. The scenario is that the user already made 2 requests and made two new requests
    within 2 seconds. And the requests from the user went to two different load balancers, then to two different rate
    limiter services. As the DB already contains the count value 2, both the Rate Limiter service gets the value below
    the threshold and permits the requests. So, we are now allowing 4 requests from the same user within a minute,
    which breaks the limit of the rate limiter. This is the inconsistency problem.

    As a solution, we may use sticky session load balancing to ensure that one user’s request will always go to the
    same rate limiter service. But the problem here is that it’s not a fault-tolerant design. Because if the Rate
    limiter service is down, user requests can not be served by the system.

    We can solve this problem using locks. Once a service uses counter data from the database, it will lock it. So,
    other services can not use it before the counter data is updated. But this procedure also has its own share of
    problems. It will add an extra latency for the locking. So, we need to decide between availability and performance
    trade-offs.


Caching:

    This system may get huge benefits from caching recent active users. Servers can quickly check if the cache contains
    the counter value for the user before hitting backend servers. This should be faster than going to the DB each time.

    We may use the Write-back cache strategy by updating all counters and timestamps in cache only. Then, we can write
    to the database done at fixed intervals, e.g., 1 hour. This can ensure minimum latency added to the user’s requests,
    which should improve the Lock latency problem.

    When the server reads data, it can always hit the cache first. This will be useful when the user has hit their
    maximum limit. The rate limiter reads data without any updates.


DoS attack:

    To prevent the system from a DoS attack, we can take other strategies to rate limit by IP address or users.

    IP: In this strategy, we limit requests per IP. It might not be a good way to differentiate between normal users
    and attackers. But, it’s still better to have some protection rather than not have anything at all.

    The biggest problem with IP-based rate limiting is when multiple users share a single public IP. For example, in
    an internet cafe or smartphone, users are using the same gateway. Another problem could be, there are lots of
    IPv6 addresses available to a hacker from even one computer.

    User: After user authentication, the system will provide the user a token which the user will pass with each
    request. This will ensure that we can rate limit an API that has a valid authentication token. But the problem is
    that we have to rate-limit the login API itself.

    The weakness of this strategy would be that a hacker can perform a denial of service attack against a user by
    providing wrong credentials up to the limit. After that, the actual user may not be able to log in.

    So, we can use both the approach together. However, this may result in more cache entries with more details per
    entry. So, we would need more memory and storage.


Conclusion:

    A rate limiter may limit the number of events an entity (user, IP, etc.) can perform in a time window. For example,
    a user can send only five requests per minute. We can shard based the data on the ‘UserID’ to distribute the
    user’s data. We should use Consistent Hashing for fault tolerance and replication. We can have different rate
    limiters for different APIs for each user or IP.

    The task of a rate limiter is to limit the number of requests to or from a system. Rate limiting is used most often
    to limit the number of incoming requests from the user in order to prevent DoS attacks. The system can enforce it
    based on IP address, user account, etc.

    We should be careful about the fact that normal DoS attacks may be prevented by rate-limiting. But in the case of
    distributed DoS attack, it may not be enough.